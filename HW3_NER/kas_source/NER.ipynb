{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (localization & Classification)\n",
    "\n",
    "In this assignment, you will implement a named entity recognition (NER) system. The goal of NER is to identify and classify named entities in a text. For example, in the sentence \"I went to the University of Illinois at Urbana-Champaign\", the named entities are \"University of Illinois at Urbana-Champaign\" (ORGANIZATION) and \"Urbana-Champaign\" (LOCATION). The NER system should be able to identify and classify the named entities in a text.\n",
    "\n",
    "^ generated by copilot. Wow it used the repo name to generate the location... wild.\n",
    "\n",
    "* RUFES https://tac.nist.gov/2020/KBP/RUFES/index.html\n",
    "* Task description paper: https://blender.cs.illinois.edu/paper/kbp2020overview.pdf\n",
    "\n",
    "### Todo: \n",
    "\n",
    "#### Train\n",
    "* Refer to Section 5 for some code of baseline models.\n",
    "* Please use the gold mentions in the tab file for trainig.\n",
    "\n",
    "#### Evaluate\n",
    "* ONLY **entity typing subtask**.\n",
    "* You need to use the extracted mentions that we provided. In real-world applications, gold mentions are typically not given.\n",
    "* Generate a prediction file and use the official scorer to compute a score.\n",
    "\n",
    "* Evaluation: https://github.com/shahraj81/rufes\n",
    "\n",
    "```\n",
    "python score_submission.py -l ../../input/demo/log.txt -r demo ../../input/log_specifications.txt ../../input/demo/gold.tab ../../input/demo/system_output.tab ../../input/demo/scores/\n",
    "\n",
    "```\n",
    "\n",
    "#### Other tools\n",
    "* You can use any tools you want to implement your system. For example, you can use the Stanford CoreNLP toolkit to extract mentions and use the AllenNLP toolkit to train a model.\n",
    "\n",
    "* Fine-grained entity recognition: https://github.com/xiaoling/figer\n",
    "* Ultra-fine grained entity typing: https://homes.cs.washington.edu/~eunsol/open_entity.html\n",
    "* Neural Entity Typing with Knowledge Attention: https://github.com/thunlp/KNET\n",
    "\n",
    "#### Submission\n",
    "* A report describing your methods, results, and findings.\n",
    "* The code. The code should include a README.md with the environment and running instructions and at least a “predictions.tab” file for your predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output format\n",
    "\n",
    "* one or more of `name` == `NAM`, `nominal`, and/or `pronominal mentions`\n",
    "* Classify into \n",
    "\n",
    "* 3 sub-tasks: mention extraction, coreference resolution, and entity typing.\n",
    "\n",
    "\n",
    "NIST\tannotation-0\trich\t20121213_WAPO_89fb0ad94e02f710be5c6a3aa2f71402:46-49\tthe rich\tPER\tNOM\t1.0\n",
    "\n",
    "```\n",
    "NIST\tannotation-1\tMitt Romney\t20111231_WAPO_2ee2b1ca-33d9-11e1-a274-61fcdeecc5f5:153-164\tMitt Romney\tPER;PER.Politician\tNAM1\t1.0\n",
    "NIST\tannotation-2\tObama\t20111231_WAPO_2ee2b1ca-33d9-11e1-a274-61fcdeecc5f5:1098-1102\t    Obama\tPER;PER.Politician;PER.Politician.HeadOfGovernment\tNAM\t1.1\n",
    "NIST\tannotation-3\tMitt Romney\t20111231_WAPO_2ee2b1ca-33d9-11e1-a274-61fcdeecc5f5:153-164\tMitt Romney\tPER;PER.Politician1\tNAM\t1.0\n",
    "NIST\tannotation-4\tObama\t20111231_WAPO_2ee2b1ca-33d9-11e1-a274-61fcdeecc5f5:1098-1102\t    Obama\tPER;PER.Politician;PER.Politician.HeadOfGovernment1\tNAM\t1.0\n",
    "NIST\tannotation-5\tMitt Romney\t20111231_WAPO_2ee2b1ca-33d9-11e1-a274-61fcdeecc5f5:164-153\tMitt Romney\tPER;PER.Politician\tNAM\t1.0\n",
    "NIST\tannotation-6\tObama\t20111231_WAPO_2ee2b1ca-33d9-11e1-a274-61fcdeecc5f5:1098-110200\t  Obama\tPER;PER.Politician;PER.Politician.HeadOfGovernment\tNAM\t1.0\n",
    "NIST\tannotation-7\tMitt Romney\t20111231_WAPO_2ee2b1ca-33d9-11e1-a274-61fcdeecc5f5a:153-164\tMitt Romney\tPER;PER.Politician\tNAM\t1.0\n",
    "NIST\tannotation-8\tObama\t20111231_WAPO_2ee2b1ca-33d9-11e1-a274-61fcdeecc5f5:-1098-1102\t    Obama\tPER;PER.Politician;PER.Politician.HeadOfGovernment\tNAM\t1.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.pipeline.ner import DEFAULT_NER_MODEL\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\"\"\"\n",
    "CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, LOC,\n",
    "MONEY, NORP, ORDINAL, ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, WORK_OF_ART\n",
    "\"\"\"\n",
    "\n",
    "# config = {\n",
    "#    \"moves\": None,\n",
    "#    \"update_with_oracle_cut_size\": 100,\n",
    "#    \"model\": DEFAULT_NER_MODEL,\n",
    "#    \"incorrect_spans_key\": \"incorrect_spans\",\n",
    "# }\n",
    "# nlp.add_pipe(\"ner\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "test = nlp(\"This is a sentence.\")\n",
    "print(type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \n",
      "\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m            English\n",
      "\u001b[0;31mString form:\u001b[0m     <spacy.lang.en.English object at 0x7fce67ee59a0>\n",
      "\u001b[0;31mFile:\u001b[0m            ~/utils/miniconda3/envs/QA_thesis/lib/python3.8/site-packages/spacy/lang/en/__init__.py\n",
      "\u001b[0;31mSource:\u001b[0m         \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mDefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnglishDefaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClass docstring:\u001b[0m\n",
      "A text-processing pipeline. Usually you'll load this once per process,\n",
      "and pass the instance around your application.\n",
      "\n",
      "Defaults (class): Settings, data and factory methods for creating the `nlp`\n",
      "    object and processing pipeline.\n",
      "lang (str): IETF language code, such as 'en'.\n",
      "\n",
      "DOCS: https://spacy.io/api/language\n",
      "\u001b[0;31mInit docstring:\u001b[0m \n",
      "Initialise a Language object.\n",
      "\n",
      "vocab (Vocab): A `Vocab` object. If `True`, a vocab is created.\n",
      "meta (dict): Custom meta data for the Language class. Is written to by\n",
      "    models to add model meta data.\n",
      "max_length (int): Maximum number of characters in a single text. The\n",
      "    current models may run out memory on extremely long texts, due to\n",
      "    large internal allocations. You should segment these texts into\n",
      "    meaningful units, e.g. paragraphs, subsections etc, before passing\n",
      "    them to spaCy. Default maximum length is 1,000,000 charas (1mb). As\n",
      "    a rule of thumb, if all pipeline components are enabled, spaCy's\n",
      "    default models currently requires roughly 1GB of temporary memory per\n",
      "    100,000 characters in one text.\n",
      "create_tokenizer (Callable): Function that takes the nlp object and\n",
      "    returns a tokenizer.\n",
      "batch_size (int): Default batch size for pipe and evaluate.\n",
      "\n",
      "DOCS: https://spacy.io/api/language#init\n",
      "\u001b[0;31mCall docstring:\u001b[0m \n",
      "Apply the pipeline to some text. The text can span multiple sentences,\n",
      "and can contain arbitrary whitespace. Alignment into the original string\n",
      "is preserved.\n",
      "\n",
      "text (Union[str, Doc]): If `str`, the text to be processed. If `Doc`,\n",
      "    the doc will be passed directly to the pipeline, skipping\n",
      "    `Language.make_doc`.\n",
      "disable (List[str]): Names of the pipeline components to disable.\n",
      "component_cfg (Dict[str, dict]): An optional dictionary with extra\n",
      "    keyword arguments for specific components.\n",
      "RETURNS (Doc): A container for accessing the annotations.\n",
      "\n",
      "DOCS: https://spacy.io/api/language#call\n"
     ]
    }
   ],
   "source": [
    "nlp??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QA_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f87fd8ce834061e373c3d3e22a8f5a00717ff019ad11ed6356a70457615f987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
