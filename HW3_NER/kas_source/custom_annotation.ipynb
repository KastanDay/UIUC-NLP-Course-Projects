{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the articles, token by token. \n",
    "(at least the characters match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in tokens (not necessary)\n",
    "      # not necessary: token_list = ltf_to_sent.load_ltf(ltf_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from utils import ltf_to_sent\n",
    "import tqdm\n",
    "import csv\n",
    "import os\n",
    "import spacy\n",
    "import tqdm\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "def get_raw_text_from_file(filepath):\n",
    "    basepath = '/home/kastan/githubs/UIUC-NLP-Course-Projects/HW3_NER/data/training/data/ltf/'\n",
    "    filepath = os.path.join(basepath, filepath)\n",
    "    text_file = open(filepath,\"r\")\n",
    "    ltf_str = text_file.read()\n",
    "    text = ltf_to_sent.ltf_to_plaintext(ltf_str)\n",
    "    return text\n",
    "      \n",
    "def get_gold_labels():\n",
    "  gold_labels_path = '/home/kastan/githubs/UIUC-NLP-Course-Projects/HW3_NER/data/training/annotations/edl.tab'\n",
    "  lines = []\n",
    "  with open(gold_labels_path) as tsv:\n",
    "    for line in csv.reader(tsv, dialect=\"excel-tab\"): #You can also use delimiter=\"\\t\" rather than giving a dialect.\n",
    "      lines.append(line)\n",
    "  print(\"example lline:\", lines[0])\n",
    "  return lines\n",
    "\n",
    "def main():\n",
    "  labels = get_gold_labels()\n",
    "  \n",
    "  # init variables\n",
    "  first_doc_name = labels[0][3].split(':')[0]\n",
    "  last_doc_filename = first_doc_name\n",
    "  count = 0 \n",
    "  doc = nlp(get_raw_text_from_file(first_doc_name))\n",
    "  db = spacy.DocBin()\n",
    "  \n",
    "  # iterate thru all labels, saving docs as we go\n",
    "  for _, num, span_text, doc_name, ent_gold, annot_list, NAM_type, const in tqdm.tqdm(labels):\n",
    "    curr_doc_filename = doc_name.split(':')[0]\n",
    "    if curr_doc_filename != last_doc_filename:\n",
    "      # save the last spacy.doc, and load in the next spacy.doc. \n",
    "      last_doc_filename = curr_doc_filename\n",
    "      db.add(doc)\n",
    "      doc = nlp(get_raw_text_from_file(curr_doc_filename))\n",
    "    \n",
    "    # iterate thru the labels and set the spans.\n",
    "    start_char, end_char = doc_name.split(':')[1].split('-')[0], doc_name.split(':')[1].split('-')[1]\n",
    "    span = doc.char_span(start_char, end_char+1, label=ent_gold, alignment_mode=\"contract\")\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "    \n",
    "    print(\"text:\", span_text)\n",
    "    print(\"ent_gold:\", ent_gold)\n",
    "    print(doc_name)\n",
    "    print(curr_doc_filename)\n",
    "    print(start_char, end_char)\n",
    "    \n",
    "    if count > 3:\n",
    "      break\n",
    "    count += 1\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DEPRICATED::  THIS IS THE BEST WAY TO READ THE LTF FILES. \"\"\"\n",
    "\n",
    "# list all files in the directory\n",
    "basepath = '/home/kastan/githubs/UIUC-NLP-Course-Projects/HW3_NER/data/training/data/ltf/'\n",
    "import os\n",
    "for file in os.listdir(\"/home/rohit/Downloads/ltf_files/\"):\n",
    "  if file.endswith(\".ltf.xml\"):\n",
    "    filepath = os.path.join(basepath, file)\n",
    "    text_file = open(filepath,\"r\")\n",
    "    ltf_str = text_file.read()\n",
    "    \n",
    "    ### Read in plain text\n",
    "    text = ltf_to_sent.ltf_to_plaintext(ltf_str)\n",
    "    start_token = 1009\n",
    "    end_token = 1013\n",
    "    print(text[start_token : end_token+1]) # NOTE the +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Read in tokens (not necessary)\\ntoken_list = ltf_to_sent.load_ltf(ltf_str)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Read in tokens (not necessary)\n",
    "token_list = ltf_to_sent.load_ltf(ltf_str)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Example code \"\"\"\n",
    "\n",
    "db = DocBin() # create a DocBin object\n",
    "\n",
    "for text, annot in tqdm.tqdm(train): # data in previous format\n",
    "    doc = nlp.make_doc(text) # create doc object from text\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents # label the text with the ents\n",
    "    db.add(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.to_disk(\"./train.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STRETCH GOAL NOT REQUIRED: Parse mention.tab\n",
    "\n",
    "We DON'T do localization. So just iterate thru the test set and classify each token in the `mention.tab` file into one of the `ontology_types.txt` classes. \n",
    "\n",
    "```\n",
    "mention_path = \"/home/kastan/githubs/UIUC-NLP-Course-Projects/HW3_NER/kas_source/mentions.tab\"\n",
    "\n",
    "ontology_types_path = \"/home/kastan/githubs/UIUC-NLP-Course-Projects/HW3_NER/kas_source/utils/rufes/tools/rufes-evaluation/example/data/ontology_types.txt\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# mention type: “NAM” (for name mentions), “NOM” (for nominal mentions), or “PRO” (for pronominal mentions)\n",
    "# We don'\n",
    "mention_path = \"/home/kastan/githubs/UIUC-NLP-Course-Projects/HW3_NER/kas_source/mentions.tab\"\n",
    "lines = []\n",
    "with open(mention_path) as tsv:\n",
    "  for line in csv.reader(tsv, dialect=\"excel-tab\"): #You can also use delimiter=\"\\t\" rather than giving a dialect.\n",
    "    lines.append(line)\n",
    "print(\"example lline:\", lines[0])\n",
    "\n",
    "# return\n",
    "# third field\n",
    "# Field 7: mention type: “NAM” (for name mentions), “NOM” (for nominal mentions), or “PRO” (for pronominal mentions)\n",
    "# NIST\tannotation-0\tFarmers\t20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2:0-6\tU.S. farmers\tPER;PER.Farmer\tNOM\t1.0\n",
    "count = 0 \n",
    "for _, num, text, doc_name, ent_gold, annot_list, NAM_type, const in tqdm.tqdm(lines):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse edl.tab (gold training labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example lline: ['NIST', 'annotation-0', 'Farmers', '20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2:0-6', 'U.S. farmers', 'PER;PER.Farmer', 'NOM', '1.0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/4727 [00:00<00:06, 757.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Farmers\n",
      "ent_gold: U.S. farmers\n",
      "20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2:0-6\n",
      "20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2\n",
      "0 6\n",
      "text: U.S.\n",
      "ent_gold: United States\n",
      "20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2:118-121\n",
      "20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2\n",
      "118 121\n",
      "text: farmers\n",
      "ent_gold: U.S. farmers\n",
      "20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2:123-129\n",
      "20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2\n",
      "123 129\n",
      "text: They\n",
      "ent_gold: U.S. farmers\n",
      "20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2:311-314\n",
      "20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2\n",
      "311 314\n",
      "text: Bloomberg\n",
      "ent_gold: Bloomberg\n",
      "20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2:397-405\n",
      "20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2\n",
      "397 405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Field 7: mention type: “NAM” (for name mentions), “NOM” (for nominal mentions), or “PRO” (for pronominal mentions)\n",
    "# NIST\tannotation-0\tFarmers\t20120211_WAPO_a6be1e92-50de-11e1-bd4f-8a7d53f6d6c2:0-6\tU.S. farmers\tPER;PER.Farmer\tNOM\t1.0\n",
    "  ''' add ents to doc '''\n",
    "  # doc = nlp.make_doc(text)\n",
    "  # ents = []\n",
    "  # for start, end, label in annot[\"entities\"]:\n",
    "  #   span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "  #   if span is None:\n",
    "  #     print(\"Skipping entity\")\n",
    "  #   else:\n",
    "  #     ents.append(span)\n",
    "  # doc.ents = ents\n",
    "  # db.add(doc)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
